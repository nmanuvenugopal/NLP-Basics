# Tokenization

Tokenization is process of splitting or breaking up original texts into its component pieces known as "tokens". 
![image](https://github.com/nmanuvenugopal/NLP-Basics/assets/99719105/31c178c8-c638-43bb-bf7c-20816fe77a25)

Spacy is intelligent enough to distinguish the "Apple" is company or organization, U.K. is a country city or state, Â£6 million is Monetoary value in the given screenshot
![image](https://github.com/nmanuvenugopal/NLP-Basics/assets/99719105/36901f3b-db6b-4ff7-9506-e712ff50da6f)

Spacy is aslo intelligent enought to distinguish between the ordinary full stop and dot in email addresss or website.
![image](https://github.com/nmanuvenugopal/NLP-Basics/assets/99719105/6917bed9-726b-4ebc-8cc3-2a58f3cbc334)



